{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_Final_Generate_Text.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashish-Tripathy/cv_yantra_drishti/blob/master/LSTM_Final_Generate_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cOZG41O3AE3",
        "colab_type": "text"
      },
      "source": [
        "# Text Generation using LSTM\n",
        "In this following submission, I have showcased how we can replace the solution from https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/ to make our generator better.\n",
        "We have done the following:\n",
        "1. Introduced padded sequences by first using a separator to separate the text into arrays, generating input sequences out of each array element and then padding the input sequences to a decided length\n",
        "2. We have generated 500 sequence of characters from our models, though our models were best predicting shorter character sequences much better due to the way we generate input sequences\n",
        "3. Trained the model for 500 epochs\n",
        "4. Added dropout of 0.1 for each lstm layer\n",
        "5. Explored multiple separators\n",
        "6. Explored word based text genaration\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrMWK9DSy9h6",
        "colab_type": "code",
        "outputId": "5e0e69bb-77bb-48b0-f337-94347860501c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Load Larger LSTM network and generate text\n",
        "import sys\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "from keras.backend import expand_dims\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpCpxp9VY1e_",
        "colab_type": "code",
        "outputId": "2b7b097a-a59a-4272-b4c6-3e0c03962097",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD0z33UP5bcu",
        "colab_type": "text"
      },
      "source": [
        "# Reading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivsNK_5Dy_tW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = \"/content/drive/My Drive/wonderland.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02S0r0xu5gck",
        "colab_type": "text"
      },
      "source": [
        "# Character based Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_16CbnJm5mYW",
        "colab_type": "text"
      },
      "source": [
        "## Using '.' as separator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aRTG8plpDvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to create variable sequence of sentences, i will remove puntctuations except '.' which will be our separator\n",
        "import string\n",
        "for x in raw_text: \n",
        "  remove = string.punctuation\n",
        "  remove = remove.replace(\".\", \"\")\n",
        "  if x in remove:\n",
        "    raw_text = raw_text.replace(x, \"\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWclCEjvzMWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create mapping of unique chars to integers, and a reverse mapping\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOTJKy1VzPdx",
        "colab_type": "code",
        "outputId": "a6b484d4-ee5a-4619-888c-5c34010d7e0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  136958\n",
            "Total Vocab:  29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TVvgbwS6Gvm",
        "colab_type": "text"
      },
      "source": [
        "For each sequence generate sequences using per character, for eg. if we have a sequence generated by separating using '.' -  alice in the wonderland.\n",
        "For this sequence we will generate input sequence like:\n",
        "\n",
        "a\n",
        "\n",
        "al\n",
        "\n",
        "ali\n",
        "\n",
        "alic\n",
        "\n",
        "alice\n",
        "\n",
        "alice \n",
        "\n",
        "alice i\n",
        "\n",
        "alice in\n",
        "\n",
        "alice in \n",
        "\n",
        "alice in t\n",
        "\n",
        "alice in th\n",
        "\n",
        "alice in the\n",
        "\n",
        "alice in the \n",
        "\n",
        "alice in the w....... like this until 'wonderland'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVHub5KS_snG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#separate by '.'\n",
        "sequence = raw_text.split('.')\n",
        "#generate input sequence\n",
        "input_sequences = []\n",
        "for line in sequence:\n",
        "    char_encodings = ([char_to_int[char] for char in line])\n",
        "    for i in range(1, len(char_encodings)):\n",
        "      input_sequences.append(char_encodings[:i+1])\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nqAFxFV6BBhH",
        "colab": {}
      },
      "source": [
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences,   \n",
        "                      maxlen=100, padding='pre'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDfIeXYMymTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_sequence_len = max([len(x) for x in input_sequences])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6UsP8kRynr0",
        "colab_type": "code",
        "outputId": "0c2fdb9c-87d4-4ca1-9de3-ff3c0fa446c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "input_sequences"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0, ...,  0,  3, 14],\n",
              "       [ 0,  0,  0, ...,  3, 14, 11],\n",
              "       [ 0,  0,  0, ..., 14, 11,  5],\n",
              "       ...,\n",
              "       [21,  1,  3, ...,  1,  6,  3],\n",
              "       [ 1,  3, 16, ...,  6,  3, 27],\n",
              "       [ 3, 16,  6, ...,  3, 27, 21]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkRJ2mgC7MRq",
        "colab_type": "text"
      },
      "source": [
        "### Training data prapration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOZTGrYT7cz-",
        "colab_type": "text"
      },
      "source": [
        "for the same 'alice in wodeland'\n",
        "\n",
        "X = a; y = l\n",
        "\n",
        "X = al; y = i\n",
        "\n",
        "X = ali; y = c\n",
        "\n",
        "and so on...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UO5YC53lBBhS",
        "colab": {}
      },
      "source": [
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "label = np_utils.to_categorical(label)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(predictors, (len(predictors), max_sequence_len-1, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv1MiAAg75kt",
        "colab_type": "text"
      },
      "source": [
        "### Building Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX8xjmfUzbPv",
        "colab_type": "code",
        "outputId": "c143b354-25d5-438a-cc53-700e95ce651d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), dropout=0.1, return_sequences=True))\n",
        "model.add(LSTM(256,dropout=0.1))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0725 06:46:26.123198 140156570359680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0725 06:46:26.164747 140156570359680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0725 06:46:26.176585 140156570359680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0725 06:46:26.419962 140156570359680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0725 06:46:26.431972 140156570359680 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0725 06:46:27.052247 140156570359680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0725 06:46:27.074467 140156570359680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4Uviu808KEv",
        "colab_type": "text"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SKyvbLCfkyy",
        "colab_type": "code",
        "outputId": "baa6a541-3013-4799-ceb3-b4eb2297fed4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "filepath=\"/content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=100, batch_size=512, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "134982/134982 [==============================] - 82s 606us/step - loss: 2.8710\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.87101, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-01-2.8710-bigger.hdf5\n",
            "Epoch 2/100\n",
            "134982/134982 [==============================] - 79s 586us/step - loss: 2.7197\n",
            "\n",
            "Epoch 00002: loss improved from 2.87101 to 2.71972, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-02-2.7197-bigger.hdf5\n",
            "Epoch 3/100\n",
            "134982/134982 [==============================] - 79s 585us/step - loss: 2.5049\n",
            "\n",
            "Epoch 00003: loss improved from 2.71972 to 2.50494, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-03-2.5049-bigger.hdf5\n",
            "Epoch 4/100\n",
            "134982/134982 [==============================] - 80s 593us/step - loss: 2.3224\n",
            "\n",
            "Epoch 00004: loss improved from 2.50494 to 2.32241, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-04-2.3224-bigger.hdf5\n",
            "Epoch 5/100\n",
            "134982/134982 [==============================] - 80s 591us/step - loss: 2.1984\n",
            "\n",
            "Epoch 00005: loss improved from 2.32241 to 2.19837, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-05-2.1984-bigger.hdf5\n",
            "Epoch 6/100\n",
            "134982/134982 [==============================] - 80s 591us/step - loss: 2.0991\n",
            "\n",
            "Epoch 00006: loss improved from 2.19837 to 2.09906, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-06-2.0991-bigger.hdf5\n",
            "Epoch 7/100\n",
            "134982/134982 [==============================] - 80s 592us/step - loss: 2.0297\n",
            "\n",
            "Epoch 00007: loss improved from 2.09906 to 2.02975, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-07-2.0297-bigger.hdf5\n",
            "Epoch 8/100\n",
            "134982/134982 [==============================] - 80s 595us/step - loss: 1.9614\n",
            "\n",
            "Epoch 00008: loss improved from 2.02975 to 1.96145, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-08-1.9614-bigger.hdf5\n",
            "Epoch 9/100\n",
            "134982/134982 [==============================] - 81s 597us/step - loss: 1.9052\n",
            "\n",
            "Epoch 00009: loss improved from 1.96145 to 1.90516, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-09-1.9052-bigger.hdf5\n",
            "Epoch 10/100\n",
            "134982/134982 [==============================] - 81s 597us/step - loss: 1.8546\n",
            "\n",
            "Epoch 00010: loss improved from 1.90516 to 1.85461, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-10-1.8546-bigger.hdf5\n",
            "Epoch 11/100\n",
            "134982/134982 [==============================] - 81s 597us/step - loss: 1.8101\n",
            "\n",
            "Epoch 00011: loss improved from 1.85461 to 1.81008, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-11-1.8101-bigger.hdf5\n",
            "Epoch 12/100\n",
            "134982/134982 [==============================] - 81s 598us/step - loss: 1.7654\n",
            "\n",
            "Epoch 00012: loss improved from 1.81008 to 1.76542, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-12-1.7654-bigger.hdf5\n",
            "Epoch 13/100\n",
            "134982/134982 [==============================] - 80s 594us/step - loss: 1.7263\n",
            "\n",
            "Epoch 00013: loss improved from 1.76542 to 1.72628, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-13-1.7263-bigger.hdf5\n",
            "Epoch 14/100\n",
            "134982/134982 [==============================] - 81s 600us/step - loss: 1.6903\n",
            "\n",
            "Epoch 00014: loss improved from 1.72628 to 1.69029, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-14-1.6903-bigger.hdf5\n",
            "Epoch 15/100\n",
            "134982/134982 [==============================] - 81s 597us/step - loss: 1.6867\n",
            "\n",
            "Epoch 00015: loss improved from 1.69029 to 1.68673, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-15-1.6867-bigger.hdf5\n",
            "Epoch 16/100\n",
            "134982/134982 [==============================] - 81s 599us/step - loss: 1.6318\n",
            "\n",
            "Epoch 00016: loss improved from 1.68673 to 1.63178, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-16-1.6318-bigger.hdf5\n",
            "Epoch 17/100\n",
            "134982/134982 [==============================] - 80s 595us/step - loss: 1.5988\n",
            "\n",
            "Epoch 00017: loss improved from 1.63178 to 1.59877, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-17-1.5988-bigger.hdf5\n",
            "Epoch 18/100\n",
            "134982/134982 [==============================] - 81s 599us/step - loss: 1.5701\n",
            "\n",
            "Epoch 00018: loss improved from 1.59877 to 1.57008, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-18-1.5701-bigger.hdf5\n",
            "Epoch 19/100\n",
            "134982/134982 [==============================] - 80s 593us/step - loss: 1.5473\n",
            "\n",
            "Epoch 00019: loss improved from 1.57008 to 1.54731, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-19-1.5473-bigger.hdf5\n",
            "Epoch 20/100\n",
            "134982/134982 [==============================] - 81s 597us/step - loss: 1.5191\n",
            "\n",
            "Epoch 00020: loss improved from 1.54731 to 1.51907, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-20-1.5191-bigger.hdf5\n",
            "Epoch 21/100\n",
            "134982/134982 [==============================] - 80s 593us/step - loss: 1.6160\n",
            "\n",
            "Epoch 00021: loss did not improve from 1.51907\n",
            "Epoch 22/100\n",
            "134982/134982 [==============================] - 81s 603us/step - loss: 1.7868\n",
            "\n",
            "Epoch 00022: loss did not improve from 1.51907\n",
            "Epoch 23/100\n",
            "134982/134982 [==============================] - 81s 604us/step - loss: 1.6816\n",
            "\n",
            "Epoch 00023: loss did not improve from 1.51907\n",
            "Epoch 24/100\n",
            "134982/134982 [==============================] - 80s 596us/step - loss: 1.6311\n",
            "\n",
            "Epoch 00024: loss did not improve from 1.51907\n",
            "Epoch 25/100\n",
            "134982/134982 [==============================] - 81s 601us/step - loss: 1.6008\n",
            "\n",
            "Epoch 00025: loss did not improve from 1.51907\n",
            "Epoch 26/100\n",
            "134982/134982 [==============================] - 82s 606us/step - loss: 1.5725\n",
            "\n",
            "Epoch 00026: loss did not improve from 1.51907\n",
            "Epoch 27/100\n",
            "134982/134982 [==============================] - 82s 607us/step - loss: 1.5507\n",
            "\n",
            "Epoch 00027: loss did not improve from 1.51907\n",
            "Epoch 28/100\n",
            "134982/134982 [==============================] - 81s 600us/step - loss: 1.5302\n",
            "\n",
            "Epoch 00028: loss did not improve from 1.51907\n",
            "Epoch 29/100\n",
            "134982/134982 [==============================] - 82s 607us/step - loss: 1.5080\n",
            "\n",
            "Epoch 00029: loss improved from 1.51907 to 1.50799, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-29-1.5080-bigger.hdf5\n",
            "Epoch 30/100\n",
            "134982/134982 [==============================] - 81s 600us/step - loss: 1.4902\n",
            "\n",
            "Epoch 00030: loss improved from 1.50799 to 1.49023, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-30-1.4902-bigger.hdf5\n",
            "Epoch 31/100\n",
            "134982/134982 [==============================] - 81s 598us/step - loss: 1.4716\n",
            "\n",
            "Epoch 00031: loss improved from 1.49023 to 1.47159, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-31-1.4716-bigger.hdf5\n",
            "Epoch 32/100\n",
            "134982/134982 [==============================] - 82s 605us/step - loss: 1.4540\n",
            "\n",
            "Epoch 00032: loss improved from 1.47159 to 1.45397, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-32-1.4540-bigger.hdf5\n",
            "Epoch 33/100\n",
            "134982/134982 [==============================] - 82s 605us/step - loss: 1.4367\n",
            "\n",
            "Epoch 00033: loss improved from 1.45397 to 1.43668, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-33-1.4367-bigger.hdf5\n",
            "Epoch 34/100\n",
            "134982/134982 [==============================] - 81s 603us/step - loss: 1.4188\n",
            "\n",
            "Epoch 00034: loss improved from 1.43668 to 1.41881, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-34-1.4188-bigger.hdf5\n",
            "Epoch 35/100\n",
            "134982/134982 [==============================] - 81s 602us/step - loss: 1.4023\n",
            "\n",
            "Epoch 00035: loss improved from 1.41881 to 1.40232, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-35-1.4023-bigger.hdf5\n",
            "Epoch 36/100\n",
            "134982/134982 [==============================] - 82s 610us/step - loss: 1.3874\n",
            "\n",
            "Epoch 00036: loss improved from 1.40232 to 1.38739, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-36-1.3874-bigger.hdf5\n",
            "Epoch 37/100\n",
            "134982/134982 [==============================] - 82s 605us/step - loss: 1.3758\n",
            "\n",
            "Epoch 00037: loss improved from 1.38739 to 1.37576, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-37-1.3758-bigger.hdf5\n",
            "Epoch 38/100\n",
            "134982/134982 [==============================] - 82s 604us/step - loss: 1.3600\n",
            "\n",
            "Epoch 00038: loss improved from 1.37576 to 1.36000, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-38-1.3600-bigger.hdf5\n",
            "Epoch 39/100\n",
            "134982/134982 [==============================] - 81s 603us/step - loss: 1.3464\n",
            "\n",
            "Epoch 00039: loss improved from 1.36000 to 1.34642, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-39-1.3464-bigger.hdf5\n",
            "Epoch 40/100\n",
            "134982/134982 [==============================] - 81s 598us/step - loss: 1.3317\n",
            "\n",
            "Epoch 00040: loss improved from 1.34642 to 1.33174, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-40-1.3317-bigger.hdf5\n",
            "Epoch 41/100\n",
            "134982/134982 [==============================] - 81s 598us/step - loss: 1.3159\n",
            "\n",
            "Epoch 00041: loss improved from 1.33174 to 1.31590, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-41-1.3159-bigger.hdf5\n",
            "Epoch 42/100\n",
            "134982/134982 [==============================] - 81s 597us/step - loss: 1.3330\n",
            "\n",
            "Epoch 00042: loss did not improve from 1.31590\n",
            "Epoch 43/100\n",
            "134982/134982 [==============================] - 81s 597us/step - loss: 1.3003\n",
            "\n",
            "Epoch 00043: loss improved from 1.31590 to 1.30028, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-43-1.3003-bigger.hdf5\n",
            "Epoch 44/100\n",
            "134982/134982 [==============================] - 80s 596us/step - loss: 1.2824\n",
            "\n",
            "Epoch 00044: loss improved from 1.30028 to 1.28243, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-44-1.2824-bigger.hdf5\n",
            "Epoch 45/100\n",
            "134982/134982 [==============================] - 82s 605us/step - loss: 1.2684\n",
            "\n",
            "Epoch 00045: loss improved from 1.28243 to 1.26837, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-45-1.2684-bigger.hdf5\n",
            "Epoch 46/100\n",
            "134982/134982 [==============================] - 81s 600us/step - loss: 1.2535\n",
            "\n",
            "Epoch 00046: loss improved from 1.26837 to 1.25346, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-46-1.2535-bigger.hdf5\n",
            "Epoch 47/100\n",
            "134982/134982 [==============================] - 81s 600us/step - loss: 1.2433\n",
            "\n",
            "Epoch 00047: loss improved from 1.25346 to 1.24334, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-47-1.2433-bigger.hdf5\n",
            "Epoch 48/100\n",
            "134982/134982 [==============================] - 81s 600us/step - loss: 1.2307\n",
            "\n",
            "Epoch 00048: loss improved from 1.24334 to 1.23073, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-48-1.2307-bigger.hdf5\n",
            "Epoch 49/100\n",
            "134982/134982 [==============================] - 80s 596us/step - loss: 1.2192\n",
            "\n",
            "Epoch 00049: loss improved from 1.23073 to 1.21924, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-49-1.2192-bigger.hdf5\n",
            "Epoch 50/100\n",
            "134982/134982 [==============================] - 80s 593us/step - loss: 1.2046\n",
            "\n",
            "Epoch 00050: loss improved from 1.21924 to 1.20457, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-50-1.2046-bigger.hdf5\n",
            "Epoch 51/100\n",
            "134982/134982 [==============================] - 81s 598us/step - loss: 1.1925\n",
            "\n",
            "Epoch 00051: loss improved from 1.20457 to 1.19251, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-51-1.1925-bigger.hdf5\n",
            "Epoch 52/100\n",
            "134982/134982 [==============================] - 81s 602us/step - loss: 1.1856\n",
            "\n",
            "Epoch 00052: loss improved from 1.19251 to 1.18565, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-52-1.1856-bigger.hdf5\n",
            "Epoch 53/100\n",
            "134982/134982 [==============================] - 82s 605us/step - loss: 1.1719\n",
            "\n",
            "Epoch 00053: loss improved from 1.18565 to 1.17188, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-53-1.1719-bigger.hdf5\n",
            "Epoch 54/100\n",
            "134982/134982 [==============================] - 81s 601us/step - loss: 1.1628\n",
            "\n",
            "Epoch 00054: loss improved from 1.17188 to 1.16283, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-54-1.1628-bigger.hdf5\n",
            "Epoch 55/100\n",
            "134982/134982 [==============================] - 82s 604us/step - loss: 1.1554\n",
            "\n",
            "Epoch 00055: loss improved from 1.16283 to 1.15536, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-55-1.1554-bigger.hdf5\n",
            "Epoch 56/100\n",
            "134982/134982 [==============================] - 81s 598us/step - loss: 1.1417\n",
            "\n",
            "Epoch 00056: loss improved from 1.15536 to 1.14175, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-56-1.1417-bigger.hdf5\n",
            "Epoch 57/100\n",
            "134982/134982 [==============================] - 81s 601us/step - loss: 1.1347\n",
            "\n",
            "Epoch 00057: loss improved from 1.14175 to 1.13468, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-57-1.1347-bigger.hdf5\n",
            "Epoch 58/100\n",
            "134982/134982 [==============================] - 82s 609us/step - loss: 1.1238\n",
            "\n",
            "Epoch 00058: loss improved from 1.13468 to 1.12375, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-58-1.1238-bigger.hdf5\n",
            "Epoch 59/100\n",
            "134982/134982 [==============================] - 81s 603us/step - loss: 1.1187\n",
            "\n",
            "Epoch 00059: loss improved from 1.12375 to 1.11872, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-59-1.1187-bigger.hdf5\n",
            "Epoch 60/100\n",
            "134982/134982 [==============================] - 82s 607us/step - loss: 1.1069\n",
            "\n",
            "Epoch 00060: loss improved from 1.11872 to 1.10687, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-60-1.1069-bigger.hdf5\n",
            "Epoch 61/100\n",
            "134982/134982 [==============================] - 81s 601us/step - loss: 1.0945\n",
            "\n",
            "Epoch 00061: loss improved from 1.10687 to 1.09451, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-61-1.0945-bigger.hdf5\n",
            "Epoch 62/100\n",
            "134982/134982 [==============================] - 80s 593us/step - loss: 1.0846\n",
            "\n",
            "Epoch 00062: loss improved from 1.09451 to 1.08457, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-62-1.0846-bigger.hdf5\n",
            "Epoch 63/100\n",
            "134982/134982 [==============================] - 81s 601us/step - loss: 1.0764\n",
            "\n",
            "Epoch 00063: loss improved from 1.08457 to 1.07639, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-63-1.0764-bigger.hdf5\n",
            "Epoch 64/100\n",
            "134982/134982 [==============================] - 82s 606us/step - loss: 1.0667\n",
            "\n",
            "Epoch 00064: loss improved from 1.07639 to 1.06667, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-64-1.0667-bigger.hdf5\n",
            "Epoch 65/100\n",
            "134982/134982 [==============================] - 81s 601us/step - loss: 1.0572\n",
            "\n",
            "Epoch 00065: loss improved from 1.06667 to 1.05718, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-65-1.0572-bigger.hdf5\n",
            "Epoch 66/100\n",
            "134982/134982 [==============================] - 81s 602us/step - loss: 1.0509\n",
            "\n",
            "Epoch 00066: loss improved from 1.05718 to 1.05087, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-66-1.0509-bigger.hdf5\n",
            "Epoch 67/100\n",
            "134982/134982 [==============================] - 80s 596us/step - loss: 1.0441\n",
            "\n",
            "Epoch 00067: loss improved from 1.05087 to 1.04407, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-67-1.0441-bigger.hdf5\n",
            "Epoch 68/100\n",
            "134982/134982 [==============================] - 81s 600us/step - loss: 1.0321\n",
            "\n",
            "Epoch 00068: loss improved from 1.04407 to 1.03210, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-68-1.0321-bigger.hdf5\n",
            "Epoch 69/100\n",
            "134982/134982 [==============================] - 80s 596us/step - loss: 1.0245\n",
            "\n",
            "Epoch 00069: loss improved from 1.03210 to 1.02446, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-69-1.0245-bigger.hdf5\n",
            "Epoch 70/100\n",
            "134982/134982 [==============================] - 80s 592us/step - loss: 1.0149\n",
            "\n",
            "Epoch 00070: loss improved from 1.02446 to 1.01486, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-70-1.0149-bigger.hdf5\n",
            "Epoch 71/100\n",
            "134982/134982 [==============================] - 81s 597us/step - loss: 1.0138\n",
            "\n",
            "Epoch 00071: loss improved from 1.01486 to 1.01385, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-71-1.0138-bigger.hdf5\n",
            "Epoch 72/100\n",
            "134982/134982 [==============================] - 80s 593us/step - loss: 1.0065\n",
            "\n",
            "Epoch 00072: loss improved from 1.01385 to 1.00652, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-72-1.0065-bigger.hdf5\n",
            "Epoch 73/100\n",
            "134982/134982 [==============================] - 80s 591us/step - loss: 0.9964\n",
            "\n",
            "Epoch 00073: loss improved from 1.00652 to 0.99636, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-73-0.9964-bigger.hdf5\n",
            "Epoch 74/100\n",
            "134982/134982 [==============================] - 79s 587us/step - loss: 0.9886\n",
            "\n",
            "Epoch 00074: loss improved from 0.99636 to 0.98859, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-74-0.9886-bigger.hdf5\n",
            "Epoch 75/100\n",
            "134982/134982 [==============================] - 80s 592us/step - loss: 0.9798\n",
            "\n",
            "Epoch 00075: loss improved from 0.98859 to 0.97982, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-75-0.9798-bigger.hdf5\n",
            "Epoch 76/100\n",
            "134982/134982 [==============================] - 79s 584us/step - loss: 0.9745\n",
            "\n",
            "Epoch 00076: loss improved from 0.97982 to 0.97449, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-76-0.9745-bigger.hdf5\n",
            "Epoch 77/100\n",
            "134982/134982 [==============================] - 79s 589us/step - loss: 0.9665\n",
            "\n",
            "Epoch 00077: loss improved from 0.97449 to 0.96650, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-77-0.9665-bigger.hdf5\n",
            "Epoch 78/100\n",
            "134982/134982 [==============================] - 80s 594us/step - loss: 0.9587\n",
            "\n",
            "Epoch 00078: loss improved from 0.96650 to 0.95868, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-78-0.9587-bigger.hdf5\n",
            "Epoch 79/100\n",
            "134982/134982 [==============================] - 82s 606us/step - loss: 0.9567\n",
            "\n",
            "Epoch 00079: loss improved from 0.95868 to 0.95672, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-79-0.9567-bigger.hdf5\n",
            "Epoch 80/100\n",
            "134982/134982 [==============================] - 81s 597us/step - loss: 0.9463\n",
            "\n",
            "Epoch 00080: loss improved from 0.95672 to 0.94627, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-80-0.9463-bigger.hdf5\n",
            "Epoch 81/100\n",
            "134982/134982 [==============================] - 80s 592us/step - loss: 0.9343\n",
            "\n",
            "Epoch 00081: loss improved from 0.94627 to 0.93433, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-81-0.9343-bigger.hdf5\n",
            "Epoch 82/100\n",
            "134982/134982 [==============================] - 80s 596us/step - loss: 0.9334\n",
            "\n",
            "Epoch 00082: loss improved from 0.93433 to 0.93345, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-82-0.9334-bigger.hdf5\n",
            "Epoch 83/100\n",
            "134982/134982 [==============================] - 80s 589us/step - loss: 0.9253\n",
            "\n",
            "Epoch 00083: loss improved from 0.93345 to 0.92530, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-83-0.9253-bigger.hdf5\n",
            "Epoch 84/100\n",
            "134982/134982 [==============================] - 79s 585us/step - loss: 0.9159\n",
            "\n",
            "Epoch 00084: loss improved from 0.92530 to 0.91594, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-84-0.9159-bigger.hdf5\n",
            "Epoch 85/100\n",
            "134982/134982 [==============================] - 78s 577us/step - loss: 0.9146\n",
            "\n",
            "Epoch 00085: loss improved from 0.91594 to 0.91462, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-85-0.9146-bigger.hdf5\n",
            "Epoch 86/100\n",
            "134982/134982 [==============================] - 77s 570us/step - loss: 0.9087\n",
            "\n",
            "Epoch 00086: loss improved from 0.91462 to 0.90871, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-86-0.9087-bigger.hdf5\n",
            "Epoch 87/100\n",
            "134982/134982 [==============================] - 75s 554us/step - loss: 0.9037\n",
            "\n",
            "Epoch 00087: loss improved from 0.90871 to 0.90367, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-87-0.9037-bigger.hdf5\n",
            "Epoch 88/100\n",
            "134982/134982 [==============================] - 75s 554us/step - loss: 0.8959\n",
            "\n",
            "Epoch 00088: loss improved from 0.90367 to 0.89586, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-88-0.8959-bigger.hdf5\n",
            "Epoch 89/100\n",
            "134982/134982 [==============================] - 75s 552us/step - loss: 0.8881\n",
            "\n",
            "Epoch 00089: loss improved from 0.89586 to 0.88807, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-89-0.8881-bigger.hdf5\n",
            "Epoch 90/100\n",
            "134982/134982 [==============================] - 74s 550us/step - loss: 0.8866\n",
            "\n",
            "Epoch 00090: loss improved from 0.88807 to 0.88662, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-90-0.8866-bigger.hdf5\n",
            "Epoch 91/100\n",
            "134982/134982 [==============================] - 75s 554us/step - loss: 0.8777\n",
            "\n",
            "Epoch 00091: loss improved from 0.88662 to 0.87768, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-91-0.8777-bigger.hdf5\n",
            "Epoch 92/100\n",
            "134982/134982 [==============================] - 74s 546us/step - loss: 0.8736\n",
            "\n",
            "Epoch 00092: loss improved from 0.87768 to 0.87357, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-92-0.8736-bigger.hdf5\n",
            "Epoch 93/100\n",
            "134982/134982 [==============================] - 74s 548us/step - loss: 0.8719\n",
            "\n",
            "Epoch 00093: loss improved from 0.87357 to 0.87188, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-93-0.8719-bigger.hdf5\n",
            "Epoch 94/100\n",
            "134982/134982 [==============================] - 73s 543us/step - loss: 0.8635\n",
            "\n",
            "Epoch 00094: loss improved from 0.87188 to 0.86345, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-94-0.8635-bigger.hdf5\n",
            "Epoch 95/100\n",
            "134982/134982 [==============================] - 74s 545us/step - loss: 0.8595\n",
            "\n",
            "Epoch 00095: loss improved from 0.86345 to 0.85953, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-95-0.8595-bigger.hdf5\n",
            "Epoch 96/100\n",
            "134982/134982 [==============================] - 73s 544us/step - loss: 0.8515\n",
            "\n",
            "Epoch 00096: loss improved from 0.85953 to 0.85149, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-96-0.8515-bigger.hdf5\n",
            "Epoch 97/100\n",
            "134982/134982 [==============================] - 74s 548us/step - loss: 0.8495\n",
            "\n",
            "Epoch 00097: loss improved from 0.85149 to 0.84949, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-97-0.8495-bigger.hdf5\n",
            "Epoch 98/100\n",
            "134982/134982 [==============================] - 74s 549us/step - loss: 0.8424\n",
            "\n",
            "Epoch 00098: loss improved from 0.84949 to 0.84241, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-98-0.8424-bigger.hdf5\n",
            "Epoch 99/100\n",
            "134982/134982 [==============================] - 74s 552us/step - loss: 0.8390\n",
            "\n",
            "Epoch 00099: loss improved from 0.84241 to 0.83905, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-99-0.8390-bigger.hdf5\n",
            "Epoch 100/100\n",
            "134982/134982 [==============================] - 75s 557us/step - loss: 0.8357\n",
            "\n",
            "Epoch 00100: loss improved from 0.83905 to 0.83569, saving model to /content/drive/My Drive/Colab Notebooks/EIPModels/EIP2/textGenerationWFS-100-0.8357-bigger.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7872703d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3m2oxEpGeGW",
        "colab_type": "code",
        "outputId": "ef331dc2-7496-4eb8-cb65-b6d1f7fb7b91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pattern.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(73,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "50090e48-3cd7-408d-8e40-e2bfa7c9e5f9",
        "id": "nJUufjmRMqny",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(input_sequences)-1)\n",
        "pattern = input_sequences[start][:-1]\n",
        "print (\"Seed:\")\n",
        "print ( (''.join([int_to_char[value] for value in pattern])).lstrip())\n",
        "\n",
        "for i in range(500):\n",
        "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "  x = x / float(n_vocab)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  result = int_to_char[index]\n",
        "  seq_in = [int_to_char[value] for value in pattern]\n",
        "  #result += \" \"+result\n",
        "  sys.stdout.write(result)\n",
        "  pattern = np.append(pattern,index)\n",
        "  pattern = pattern[1:len(pattern)]\n",
        "\n",
        "print (\"\\nDone.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "they cant have anything to put\n",
            "down yet before the trials be\n",
            "out hor the hanl asd tuleby mtsk\n",
            "at if hauhen it a large caner sdtsing sain the had\n",
            "nov a lowt of taid the had\n",
            "notw homes anice colcued wenl tomeiny sarked youd bnd atwinusly reparkasd abtkiloed atas lurglyed at the san anice comping un the\n",
            "wery dowid in rntrer suestion anlce and ie walk woure tatting about forught toeep and hlgsgit the whougst whers tuch a crpk to the\n",
            "whough you   little before so the wail wo her fanl a darcupill mabel turning eurwtiyer the when of tersehty inm hit tostling abo\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dGW0SpR8g2r",
        "colab_type": "text"
      },
      "source": [
        "### Genarated text:\n",
        "\n",
        "seed: \n",
        "\n",
        "they cant have anything to put\n",
        "down yet before the trials be\n",
        "\n",
        "Text generated: \n",
        "\n",
        "out hor the hanl asd tuleby mtsk\n",
        "at if hauhen it a large caner sdtsing sain the had\n",
        "nov a lowt of taid the had\n",
        "notw homes anice colcued wenl tomeiny sarked youd bnd atwinusly reparkasd abtkiloed atas lurglyed at the san anice comping un the\n",
        "wery dowid in rntrer suestion anlce and ie walk woure tatting about forught toeep and hlgsgit the whougst whers tuch a crpk to the\n",
        "whough you   little before so the wail wo her fanl a darcupill mabel turning eurwtiyer the when of tersehty inm hit tostling abo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQqIPG4w9Hbx",
        "colab_type": "text"
      },
      "source": [
        "## Using 30th space as separator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qTAyXGB52rlt",
        "colab": {}
      },
      "source": [
        "import string\n",
        "for x in raw_text: \n",
        "    if x in string.punctuation:\n",
        "        raw_text = raw_text.replace(x, \"\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XCxUgWuF2rl0",
        "colab": {}
      },
      "source": [
        "# create mapping of unique chars to integers, and a reverse mapping\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7176138c-85d4-45c3-99cd-b438543d37b8",
        "id": "xDenG7Xd2rl7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  135970\n",
            "Total Vocab:  28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aMklvaAy2rmP",
        "colab": {}
      },
      "source": [
        "span = 30\n",
        "sequence = raw_text.split(' ')\n",
        "sequence = [\" \".join(sequence[i:i+span]) for i in range(0, len(sequence), span)] #splitting on every 30th space"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vpaiv-eY2rmU",
        "colab": {}
      },
      "source": [
        "input_sequences = []\n",
        "for line in sequence:\n",
        "    char_encodings = ([char_to_int[char] for char in line])#tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(char_encodings)):\n",
        "      input_sequences.append(char_encodings[:i+1])\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmDGO5pPD8uw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_seq = np.array(pad_sequences(input_sequences,maxlen=100, padding='pre'))\n",
        "max_sequence_len = max([len(x) for x in input_seq])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCD2DVhSvv-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictors, label = input_seq[:,:-1],input_seq[:,-1]\n",
        "label = np_utils.to_categorical(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OKeM7EpDIQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(predictors, (len(predictors), max_sequence_len-1, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4yPZndX52rm0",
        "colab": {}
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), dropout=0.1, return_sequences=True))\n",
        "model.add(LSTM(256,dropout=0.1))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3fa84d21-96d3-4bac-e543-a02dadc36022",
        "id": "cynw3FNT2rm5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "filepath=\"/content/drive/My Drive/EIP2/Best models/textGeneration-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=100, batch_size=512, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "134301/134301 [==============================] - 79s 586us/step - loss: 2.8772\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.87725, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-01-2.8772-bigger.hdf5\n",
            "Epoch 2/100\n",
            "134301/134301 [==============================] - 77s 575us/step - loss: 2.7263\n",
            "\n",
            "Epoch 00002: loss improved from 2.87725 to 2.72626, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-02-2.7263-bigger.hdf5\n",
            "Epoch 3/100\n",
            "134301/134301 [==============================] - 78s 577us/step - loss: 2.5175\n",
            "\n",
            "Epoch 00003: loss improved from 2.72626 to 2.51750, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-03-2.5175-bigger.hdf5\n",
            "Epoch 4/100\n",
            "134301/134301 [==============================] - 78s 581us/step - loss: 2.3335\n",
            "\n",
            "Epoch 00004: loss improved from 2.51750 to 2.33353, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-04-2.3335-bigger.hdf5\n",
            "Epoch 5/100\n",
            "134301/134301 [==============================] - 78s 578us/step - loss: 2.2061\n",
            "\n",
            "Epoch 00005: loss improved from 2.33353 to 2.20609, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-05-2.2061-bigger.hdf5\n",
            "Epoch 6/100\n",
            "134301/134301 [==============================] - 78s 580us/step - loss: 2.1049\n",
            "\n",
            "Epoch 00006: loss improved from 2.20609 to 2.10486, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-06-2.1049-bigger.hdf5\n",
            "Epoch 7/100\n",
            "134301/134301 [==============================] - 77s 575us/step - loss: 2.0255\n",
            "\n",
            "Epoch 00007: loss improved from 2.10486 to 2.02550, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-07-2.0255-bigger.hdf5\n",
            "Epoch 8/100\n",
            "134301/134301 [==============================] - 77s 572us/step - loss: 1.9551\n",
            "\n",
            "Epoch 00008: loss improved from 2.02550 to 1.95510, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-08-1.9551-bigger.hdf5\n",
            "Epoch 9/100\n",
            "134301/134301 [==============================] - 77s 574us/step - loss: 1.9013\n",
            "\n",
            "Epoch 00009: loss improved from 1.95510 to 1.90130, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-09-1.9013-bigger.hdf5\n",
            "Epoch 10/100\n",
            "134301/134301 [==============================] - 78s 577us/step - loss: 1.8532\n",
            "\n",
            "Epoch 00010: loss improved from 1.90130 to 1.85325, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-10-1.8532-bigger.hdf5\n",
            "Epoch 11/100\n",
            "134301/134301 [==============================] - 77s 575us/step - loss: 1.8053\n",
            "\n",
            "Epoch 00011: loss improved from 1.85325 to 1.80532, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-11-1.8053-bigger.hdf5\n",
            "Epoch 12/100\n",
            "134301/134301 [==============================] - 78s 578us/step - loss: 1.7667\n",
            "\n",
            "Epoch 00012: loss improved from 1.80532 to 1.76671, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-12-1.7667-bigger.hdf5\n",
            "Epoch 13/100\n",
            "134301/134301 [==============================] - 77s 577us/step - loss: 1.7297\n",
            "\n",
            "Epoch 00013: loss improved from 1.76671 to 1.72972, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-13-1.7297-bigger.hdf5\n",
            "Epoch 14/100\n",
            "134301/134301 [==============================] - 78s 578us/step - loss: 1.6962\n",
            "\n",
            "Epoch 00014: loss improved from 1.72972 to 1.69619, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-14-1.6962-bigger.hdf5\n",
            "Epoch 15/100\n",
            "134301/134301 [==============================] - 77s 574us/step - loss: 1.6660\n",
            "\n",
            "Epoch 00015: loss improved from 1.69619 to 1.66603, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-15-1.6660-bigger.hdf5\n",
            "Epoch 16/100\n",
            "134301/134301 [==============================] - 77s 572us/step - loss: 1.6356\n",
            "\n",
            "Epoch 00016: loss improved from 1.66603 to 1.63560, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-16-1.6356-bigger.hdf5\n",
            "Epoch 17/100\n",
            "134301/134301 [==============================] - 77s 572us/step - loss: 1.6075\n",
            "\n",
            "Epoch 00017: loss improved from 1.63560 to 1.60747, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-17-1.6075-bigger.hdf5\n",
            "Epoch 18/100\n",
            "134301/134301 [==============================] - 77s 575us/step - loss: 1.5784\n",
            "\n",
            "Epoch 00018: loss improved from 1.60747 to 1.57844, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-18-1.5784-bigger.hdf5\n",
            "Epoch 19/100\n",
            "134301/134301 [==============================] - 77s 570us/step - loss: 1.5524\n",
            "\n",
            "Epoch 00019: loss improved from 1.57844 to 1.55236, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-19-1.5524-bigger.hdf5\n",
            "Epoch 20/100\n",
            "134301/134301 [==============================] - 77s 573us/step - loss: 1.5320\n",
            "\n",
            "Epoch 00020: loss improved from 1.55236 to 1.53204, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-20-1.5320-bigger.hdf5\n",
            "Epoch 21/100\n",
            "134301/134301 [==============================] - 77s 572us/step - loss: 1.5056\n",
            "\n",
            "Epoch 00021: loss improved from 1.53204 to 1.50564, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-21-1.5056-bigger.hdf5\n",
            "Epoch 22/100\n",
            "134301/134301 [==============================] - 77s 574us/step - loss: 1.4849\n",
            "\n",
            "Epoch 00022: loss improved from 1.50564 to 1.48492, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-22-1.4849-bigger.hdf5\n",
            "Epoch 23/100\n",
            "134301/134301 [==============================] - 77s 570us/step - loss: 1.4662\n",
            "\n",
            "Epoch 00023: loss improved from 1.48492 to 1.46620, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-23-1.4662-bigger.hdf5\n",
            "Epoch 24/100\n",
            "134301/134301 [==============================] - 77s 571us/step - loss: 1.4429\n",
            "\n",
            "Epoch 00024: loss improved from 1.46620 to 1.44293, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-24-1.4429-bigger.hdf5\n",
            "Epoch 25/100\n",
            "134301/134301 [==============================] - 77s 570us/step - loss: 1.4208\n",
            "\n",
            "Epoch 00025: loss improved from 1.44293 to 1.42080, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-25-1.4208-bigger.hdf5\n",
            "Epoch 26/100\n",
            "134301/134301 [==============================] - 78s 577us/step - loss: 1.4004\n",
            "\n",
            "Epoch 00026: loss improved from 1.42080 to 1.40041, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-26-1.4004-bigger.hdf5\n",
            "Epoch 27/100\n",
            "134301/134301 [==============================] - 77s 573us/step - loss: 1.3810\n",
            "\n",
            "Epoch 00027: loss improved from 1.40041 to 1.38101, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-27-1.3810-bigger.hdf5\n",
            "Epoch 28/100\n",
            "134301/134301 [==============================] - 77s 574us/step - loss: 1.3625\n",
            "\n",
            "Epoch 00028: loss improved from 1.38101 to 1.36245, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-28-1.3625-bigger.hdf5\n",
            "Epoch 29/100\n",
            "134301/134301 [==============================] - 77s 571us/step - loss: 1.3418\n",
            "\n",
            "Epoch 00029: loss improved from 1.36245 to 1.34175, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-29-1.3418-bigger.hdf5\n",
            "Epoch 30/100\n",
            "134301/134301 [==============================] - 77s 575us/step - loss: 1.3274\n",
            "\n",
            "Epoch 00030: loss improved from 1.34175 to 1.32738, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-30-1.3274-bigger.hdf5\n",
            "Epoch 31/100\n",
            "134301/134301 [==============================] - 77s 571us/step - loss: 1.3119\n",
            "\n",
            "Epoch 00031: loss improved from 1.32738 to 1.31187, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-31-1.3119-bigger.hdf5\n",
            "Epoch 32/100\n",
            "134301/134301 [==============================] - 77s 572us/step - loss: 1.2921\n",
            "\n",
            "Epoch 00032: loss improved from 1.31187 to 1.29208, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-32-1.2921-bigger.hdf5\n",
            "Epoch 33/100\n",
            "134301/134301 [==============================] - 77s 570us/step - loss: 1.2734\n",
            "\n",
            "Epoch 00033: loss improved from 1.29208 to 1.27342, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-33-1.2734-bigger.hdf5\n",
            "Epoch 34/100\n",
            "134301/134301 [==============================] - 77s 574us/step - loss: 1.2582\n",
            "\n",
            "Epoch 00034: loss improved from 1.27342 to 1.25820, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-34-1.2582-bigger.hdf5\n",
            "Epoch 35/100\n",
            "134301/134301 [==============================] - 77s 573us/step - loss: 1.2425\n",
            "\n",
            "Epoch 00035: loss improved from 1.25820 to 1.24251, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-35-1.2425-bigger.hdf5\n",
            "Epoch 36/100\n",
            "134301/134301 [==============================] - 77s 571us/step - loss: 1.2277\n",
            "\n",
            "Epoch 00036: loss improved from 1.24251 to 1.22766, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-36-1.2277-bigger.hdf5\n",
            "Epoch 37/100\n",
            "134301/134301 [==============================] - 77s 574us/step - loss: 1.2148\n",
            "\n",
            "Epoch 00037: loss improved from 1.22766 to 1.21479, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-37-1.2148-bigger.hdf5\n",
            "Epoch 38/100\n",
            "134301/134301 [==============================] - 77s 573us/step - loss: 1.1999\n",
            "\n",
            "Epoch 00038: loss improved from 1.21479 to 1.19994, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-38-1.1999-bigger.hdf5\n",
            "Epoch 39/100\n",
            "134301/134301 [==============================] - 76s 565us/step - loss: 1.1908\n",
            "\n",
            "Epoch 00039: loss improved from 1.19994 to 1.19077, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-39-1.1908-bigger.hdf5\n",
            "Epoch 40/100\n",
            "134301/134301 [==============================] - 76s 565us/step - loss: 1.1690\n",
            "\n",
            "Epoch 00040: loss improved from 1.19077 to 1.16898, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-40-1.1690-bigger.hdf5\n",
            "Epoch 41/100\n",
            "134301/134301 [==============================] - 77s 571us/step - loss: 1.1556\n",
            "\n",
            "Epoch 00041: loss improved from 1.16898 to 1.15562, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-41-1.1556-bigger.hdf5\n",
            "Epoch 42/100\n",
            "134301/134301 [==============================] - 77s 573us/step - loss: 1.1385\n",
            "\n",
            "Epoch 00042: loss improved from 1.15562 to 1.13851, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-42-1.1385-bigger.hdf5\n",
            "Epoch 43/100\n",
            "134301/134301 [==============================] - 76s 568us/step - loss: 1.1333\n",
            "\n",
            "Epoch 00043: loss improved from 1.13851 to 1.13334, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-43-1.1333-bigger.hdf5\n",
            "Epoch 44/100\n",
            "134301/134301 [==============================] - 76s 568us/step - loss: 1.1166\n",
            "\n",
            "Epoch 00044: loss improved from 1.13334 to 1.11660, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-44-1.1166-bigger.hdf5\n",
            "Epoch 45/100\n",
            "134301/134301 [==============================] - 76s 568us/step - loss: 1.1055\n",
            "\n",
            "Epoch 00045: loss improved from 1.11660 to 1.10549, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-45-1.1055-bigger.hdf5\n",
            "Epoch 46/100\n",
            "134301/134301 [==============================] - 77s 575us/step - loss: 1.0899\n",
            "\n",
            "Epoch 00046: loss improved from 1.10549 to 1.08991, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-46-1.0899-bigger.hdf5\n",
            "Epoch 47/100\n",
            "134301/134301 [==============================] - 77s 571us/step - loss: 1.0746\n",
            "\n",
            "Epoch 00047: loss improved from 1.08991 to 1.07461, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-47-1.0746-bigger.hdf5\n",
            "Epoch 48/100\n",
            "134301/134301 [==============================] - 76s 570us/step - loss: 1.0662\n",
            "\n",
            "Epoch 00048: loss improved from 1.07461 to 1.06624, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-48-1.0662-bigger.hdf5\n",
            "Epoch 49/100\n",
            "134301/134301 [==============================] - 76s 569us/step - loss: 1.0556\n",
            "\n",
            "Epoch 00049: loss improved from 1.06624 to 1.05556, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-49-1.0556-bigger.hdf5\n",
            "Epoch 50/100\n",
            "134301/134301 [==============================] - 77s 574us/step - loss: 1.0403\n",
            "\n",
            "Epoch 00050: loss improved from 1.05556 to 1.04034, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-50-1.0403-bigger.hdf5\n",
            "Epoch 51/100\n",
            "134301/134301 [==============================] - 77s 571us/step - loss: 1.0341\n",
            "\n",
            "Epoch 00051: loss improved from 1.04034 to 1.03409, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-51-1.0341-bigger.hdf5\n",
            "Epoch 52/100\n",
            "134301/134301 [==============================] - 77s 572us/step - loss: 1.0183\n",
            "\n",
            "Epoch 00052: loss improved from 1.03409 to 1.01825, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-52-1.0183-bigger.hdf5\n",
            "Epoch 53/100\n",
            "134301/134301 [==============================] - 77s 575us/step - loss: 1.0060\n",
            "\n",
            "Epoch 00053: loss improved from 1.01825 to 1.00600, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-53-1.0060-bigger.hdf5\n",
            "Epoch 54/100\n",
            "134301/134301 [==============================] - 78s 578us/step - loss: 0.9970\n",
            "\n",
            "Epoch 00054: loss improved from 1.00600 to 0.99704, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-54-0.9970-bigger.hdf5\n",
            "Epoch 55/100\n",
            "134301/134301 [==============================] - 77s 574us/step - loss: 0.9892\n",
            "\n",
            "Epoch 00055: loss improved from 0.99704 to 0.98920, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-55-0.9892-bigger.hdf5\n",
            "Epoch 56/100\n",
            "134301/134301 [==============================] - 77s 574us/step - loss: 0.9779\n",
            "\n",
            "Epoch 00056: loss improved from 0.98920 to 0.97794, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-56-0.9779-bigger.hdf5\n",
            "Epoch 57/100\n",
            "134301/134301 [==============================] - 77s 574us/step - loss: 0.9725\n",
            "\n",
            "Epoch 00057: loss improved from 0.97794 to 0.97248, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-57-0.9725-bigger.hdf5\n",
            "Epoch 58/100\n",
            "134301/134301 [==============================] - 77s 575us/step - loss: 0.9636\n",
            "\n",
            "Epoch 00058: loss improved from 0.97248 to 0.96358, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-58-0.9636-bigger.hdf5\n",
            "Epoch 59/100\n",
            "134301/134301 [==============================] - 76s 569us/step - loss: 0.9471\n",
            "\n",
            "Epoch 00059: loss improved from 0.96358 to 0.94710, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-59-0.9471-bigger.hdf5\n",
            "Epoch 60/100\n",
            "134301/134301 [==============================] - 77s 572us/step - loss: 0.9388\n",
            "\n",
            "Epoch 00060: loss improved from 0.94710 to 0.93879, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-60-0.9388-bigger.hdf5\n",
            "Epoch 61/100\n",
            "134301/134301 [==============================] - 77s 574us/step - loss: 0.9310\n",
            "\n",
            "Epoch 00061: loss improved from 0.93879 to 0.93097, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-61-0.9310-bigger.hdf5\n",
            "Epoch 62/100\n",
            "134301/134301 [==============================] - 77s 571us/step - loss: 0.9160\n",
            "\n",
            "Epoch 00062: loss improved from 0.93097 to 0.91600, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-62-0.9160-bigger.hdf5\n",
            "Epoch 63/100\n",
            "134301/134301 [==============================] - 76s 569us/step - loss: 0.9087\n",
            "\n",
            "Epoch 00063: loss improved from 0.91600 to 0.90872, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-63-0.9087-bigger.hdf5\n",
            "Epoch 64/100\n",
            "134301/134301 [==============================] - 77s 572us/step - loss: 0.9033\n",
            "\n",
            "Epoch 00064: loss improved from 0.90872 to 0.90328, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-64-0.9033-bigger.hdf5\n",
            "Epoch 65/100\n",
            "134301/134301 [==============================] - 77s 574us/step - loss: 0.8891\n",
            "\n",
            "Epoch 00065: loss improved from 0.90328 to 0.88906, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-65-0.8891-bigger.hdf5\n",
            "Epoch 66/100\n",
            "134301/134301 [==============================] - 78s 579us/step - loss: 0.8860\n",
            "\n",
            "Epoch 00066: loss improved from 0.88906 to 0.88599, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-66-0.8860-bigger.hdf5\n",
            "Epoch 67/100\n",
            "134301/134301 [==============================] - 77s 575us/step - loss: 0.8740\n",
            "\n",
            "Epoch 00067: loss improved from 0.88599 to 0.87404, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-67-0.8740-bigger.hdf5\n",
            "Epoch 68/100\n",
            "134301/134301 [==============================] - 77s 576us/step - loss: 0.8649\n",
            "\n",
            "Epoch 00068: loss improved from 0.87404 to 0.86488, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-68-0.8649-bigger.hdf5\n",
            "Epoch 69/100\n",
            "134301/134301 [==============================] - 78s 578us/step - loss: 0.8651\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.86488\n",
            "Epoch 70/100\n",
            "134301/134301 [==============================] - 74s 554us/step - loss: 0.8486\n",
            "\n",
            "Epoch 00070: loss improved from 0.86488 to 0.84860, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-70-0.8486-bigger.hdf5\n",
            "Epoch 71/100\n",
            "134301/134301 [==============================] - 72s 539us/step - loss: 0.8460\n",
            "\n",
            "Epoch 00071: loss improved from 0.84860 to 0.84603, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-71-0.8460-bigger.hdf5\n",
            "Epoch 72/100\n",
            "134301/134301 [==============================] - 72s 538us/step - loss: 0.8363\n",
            "\n",
            "Epoch 00072: loss improved from 0.84603 to 0.83630, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-72-0.8363-bigger.hdf5\n",
            "Epoch 73/100\n",
            "134301/134301 [==============================] - 72s 536us/step - loss: 0.8249\n",
            "\n",
            "Epoch 00073: loss improved from 0.83630 to 0.82486, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-73-0.8249-bigger.hdf5\n",
            "Epoch 74/100\n",
            "134301/134301 [==============================] - 72s 539us/step - loss: 0.8204\n",
            "\n",
            "Epoch 00074: loss improved from 0.82486 to 0.82039, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-74-0.8204-bigger.hdf5\n",
            "Epoch 75/100\n",
            "134301/134301 [==============================] - 72s 536us/step - loss: 0.8134\n",
            "\n",
            "Epoch 00075: loss improved from 0.82039 to 0.81344, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-75-0.8134-bigger.hdf5\n",
            "Epoch 76/100\n",
            "134301/134301 [==============================] - 72s 535us/step - loss: 0.8041\n",
            "\n",
            "Epoch 00076: loss improved from 0.81344 to 0.80411, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-76-0.8041-bigger.hdf5\n",
            "Epoch 77/100\n",
            "134301/134301 [==============================] - 72s 534us/step - loss: 0.7912\n",
            "\n",
            "Epoch 00077: loss improved from 0.80411 to 0.79122, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-77-0.7912-bigger.hdf5\n",
            "Epoch 78/100\n",
            "134301/134301 [==============================] - 72s 534us/step - loss: 0.7925\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.79122\n",
            "Epoch 79/100\n",
            "134301/134301 [==============================] - 72s 534us/step - loss: 0.7855\n",
            "\n",
            "Epoch 00079: loss improved from 0.79122 to 0.78546, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-79-0.7855-bigger.hdf5\n",
            "Epoch 80/100\n",
            "134301/134301 [==============================] - 71s 531us/step - loss: 0.7816\n",
            "\n",
            "Epoch 00080: loss improved from 0.78546 to 0.78160, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-80-0.7816-bigger.hdf5\n",
            "Epoch 81/100\n",
            "134301/134301 [==============================] - 72s 532us/step - loss: 0.7713\n",
            "\n",
            "Epoch 00081: loss improved from 0.78160 to 0.77126, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-81-0.7713-bigger.hdf5\n",
            "Epoch 82/100\n",
            "134301/134301 [==============================] - 72s 533us/step - loss: 0.7650\n",
            "\n",
            "Epoch 00082: loss improved from 0.77126 to 0.76502, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-82-0.7650-bigger.hdf5\n",
            "Epoch 83/100\n",
            "134301/134301 [==============================] - 72s 535us/step - loss: 0.7639\n",
            "\n",
            "Epoch 00083: loss improved from 0.76502 to 0.76395, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-83-0.7639-bigger.hdf5\n",
            "Epoch 84/100\n",
            "134301/134301 [==============================] - 71s 532us/step - loss: 0.7521\n",
            "\n",
            "Epoch 00084: loss improved from 0.76395 to 0.75215, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-84-0.7521-bigger.hdf5\n",
            "Epoch 85/100\n",
            "134301/134301 [==============================] - 71s 530us/step - loss: 0.7497\n",
            "\n",
            "Epoch 00085: loss improved from 0.75215 to 0.74968, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-85-0.7497-bigger.hdf5\n",
            "Epoch 86/100\n",
            "134301/134301 [==============================] - 71s 532us/step - loss: 0.7356\n",
            "\n",
            "Epoch 00086: loss improved from 0.74968 to 0.73559, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-86-0.7356-bigger.hdf5\n",
            "Epoch 87/100\n",
            "134301/134301 [==============================] - 71s 531us/step - loss: 0.7334\n",
            "\n",
            "Epoch 00087: loss improved from 0.73559 to 0.73343, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-87-0.7334-bigger.hdf5\n",
            "Epoch 88/100\n",
            "134301/134301 [==============================] - 71s 531us/step - loss: 0.7254\n",
            "\n",
            "Epoch 00088: loss improved from 0.73343 to 0.72542, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-88-0.7254-bigger.hdf5\n",
            "Epoch 89/100\n",
            "134301/134301 [==============================] - 71s 530us/step - loss: 0.7227\n",
            "\n",
            "Epoch 00089: loss improved from 0.72542 to 0.72272, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-89-0.7227-bigger.hdf5\n",
            "Epoch 90/100\n",
            "134301/134301 [==============================] - 71s 529us/step - loss: 0.7159\n",
            "\n",
            "Epoch 00090: loss improved from 0.72272 to 0.71594, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-90-0.7159-bigger.hdf5\n",
            "Epoch 91/100\n",
            "134301/134301 [==============================] - 71s 530us/step - loss: 0.7081\n",
            "\n",
            "Epoch 00091: loss improved from 0.71594 to 0.70806, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-91-0.7081-bigger.hdf5\n",
            "Epoch 92/100\n",
            "134301/134301 [==============================] - 72s 533us/step - loss: 0.7101\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.70806\n",
            "Epoch 93/100\n",
            "134301/134301 [==============================] - 71s 529us/step - loss: 0.7028\n",
            "\n",
            "Epoch 00093: loss improved from 0.70806 to 0.70280, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-93-0.7028-bigger.hdf5\n",
            "Epoch 94/100\n",
            "134301/134301 [==============================] - 71s 529us/step - loss: 0.6933\n",
            "\n",
            "Epoch 00094: loss improved from 0.70280 to 0.69331, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-94-0.6933-bigger.hdf5\n",
            "Epoch 95/100\n",
            "134301/134301 [==============================] - 71s 529us/step - loss: 0.6826\n",
            "\n",
            "Epoch 00095: loss improved from 0.69331 to 0.68256, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-95-0.6826-bigger.hdf5\n",
            "Epoch 96/100\n",
            "134301/134301 [==============================] - 71s 532us/step - loss: 0.6863\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.68256\n",
            "Epoch 97/100\n",
            "134301/134301 [==============================] - 71s 528us/step - loss: 0.6817\n",
            "\n",
            "Epoch 00097: loss improved from 0.68256 to 0.68170, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-97-0.6817-bigger.hdf5\n",
            "Epoch 98/100\n",
            "134301/134301 [==============================] - 71s 528us/step - loss: 0.6734\n",
            "\n",
            "Epoch 00098: loss improved from 0.68170 to 0.67336, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-98-0.6734-bigger.hdf5\n",
            "Epoch 99/100\n",
            "134301/134301 [==============================] - 71s 529us/step - loss: 0.6737\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.67336\n",
            "Epoch 100/100\n",
            "134301/134301 [==============================] - 72s 534us/step - loss: 0.6618\n",
            "\n",
            "Epoch 00100: loss improved from 0.67336 to 0.66178, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-100-0.6618-bigger.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff59621aeb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEtQT_Pc0bEd",
        "colab_type": "code",
        "outputId": "b9352faf-c5ad-4078-c432-64a604dae1ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/drive/My Drive/EIP2/Best models/textGeneration-100-0.6618-bigger.hdf5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0727 14:27:09.339074 140701900285824 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0727 14:27:09.380603 140701900285824 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0727 14:27:09.388442 140701900285824 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0727 14:27:09.655366 140701900285824 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0727 14:27:09.671676 140701900285824 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0727 14:27:10.391944 140701900285824 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0727 14:27:13.405679 140701900285824 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0727 14:27:13.570689 140701900285824 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e5648255-1fee-41ae-bfd4-652d0af95d54",
        "id": "V0R-dYMm2rnP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(input_sequences)-1)\n",
        "pattern = predictors[start]\n",
        "print (\"Seed:\")\n",
        "print ( (''.join([int_to_char[value] for value in pattern])).lstrip())\n",
        "\n",
        "for i in range(500):\n",
        "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "  x = x / float(n_vocab)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  result = int_to_char[index]\n",
        "  seq_in = [int_to_char[value] for value in pattern]\n",
        "  #result += \" \"+result\n",
        "  sys.stdout.write(result)\n",
        "  pattern = np.append(pattern,index)\n",
        "  pattern = pattern[1:len(pattern)]\n",
        "\n",
        "print (\"\\nDone.\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "ly round to see if he would deny it too but the dormouse denied\n",
            "nothing being fast asleep\n",
            "\n",
            "after th\n",
            "ared like to begin anl ever toe couter of alice as hen footistoaus themr what toeak tuupid\n",
            "alice tuiet tulemer\n",
            " alice could herself in mine tur inme toeakne suisented the hedlery what ialittle hirher what ie yotklng oeaus youn nakes the waid to lert each nine tou thenish po toeep oitsle uipink forseruueset hir thadh at hol anl thes tureard doos becr the had nodeed sathahe\n",
            "\n",
            "whe slal sattuinner\n",
            "\n",
            "alice wail oe for\n",
            "hirst hererally as iatolams respoe wery toow lnowtession what iadhent whine soeedhng \n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_N6yPay-Jye",
        "colab_type": "text"
      },
      "source": [
        "<b>Seed:</b>\n",
        "ly round to see if he would deny it too but the dormouse denied\n",
        "nothing being fast asleep\n",
        "\n",
        "\n",
        "<b>Generated text: </b>\n",
        "after th\n",
        "ared like to begin anl ever toe couter of alice as hen footistoaus themr what toeak tuupid\n",
        "alice tuiet tulemer\n",
        " alice could herself in mine tur inme toeakne suisented the hedlery what ialittle hirher what ie yotklng oeaus youn nakes the waid to lert each nine tou thenish po toeep oitsle uipink forseruueset hir thadh at hol anl thes tureard doos becr the had nodeed sathahe\n",
        "\n",
        "whe slal sattuinner\n",
        "\n",
        "alice wail oe for\n",
        "hirst hererally as iatolams respoe wery toow lnowtession what iadhent whine soeedhng\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKf6zNLT-iIy",
        "colab_type": "text"
      },
      "source": [
        "## Other creative separators explored:\n",
        "\n",
        "1. random nth space to generate input sequences like the fixed sequence method used in machine learning mastery\n",
        "2. '\\n' as separator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWrYKfp-axym",
        "colab_type": "text"
      },
      "source": [
        "# Using Word based Generators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBlxVFw2_jKg",
        "colab_type": "text"
      },
      "source": [
        "Explored Word based generators which are widely used for text generation as it creates a dictionary for each word to be used, so the geneated text atleast has correct words, the model has to learn in what context to use the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF75jzX2j5pZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "for x in raw_text: \n",
        "    if x in string.punctuation:\n",
        "        raw_text = raw_text.replace(x, \"\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_exmGVGy_wA_",
        "colab_type": "text"
      },
      "source": [
        "## Using '\\n' as separator\n",
        "\n",
        "Building \\n as separator, fit a tokenizer() which forms tokens for each word in the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL1vfRpdnFD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "raw_text = raw_text.split(\"\\n\")\n",
        "tokenizer.fit_on_texts(raw_text)\n",
        "total_words = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "029wY-gmACLB",
        "colab_type": "text"
      },
      "source": [
        "## Create input sequences \n",
        "Text: Alice in Wonderland\n",
        "\n",
        "Alice\n",
        "\n",
        "Alice in\n",
        "\n",
        "Alice in Wonderland\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIoPXLTe5hyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_list = tokenizer.texts_to_sequences(raw_text)[0]\n",
        "for i in range(1, len(token_list)):\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_rbEd6tA1Ib",
        "colab_type": "text"
      },
      "source": [
        "## Padding the sequences with 0s to create fixed length sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vttkv3GLocj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences,   \n",
        "                      maxlen=max_sequence_len, padding='pre'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVWYxwvCA70F",
        "colab_type": "text"
      },
      "source": [
        "## Training data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqoemRT0rftF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "label = np_utils.to_categorical(label, num_classes=total_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qzfp7Z3BRGD",
        "colab_type": "text"
      },
      "source": [
        "## Building the model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aI4PR_hxeef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_len = max_sequence_len - 1\n",
        "model = Sequential()\n",
        "\n",
        "# Add Input Embedding Layer\n",
        "model.add(Embedding(total_words, 10, input_length=input_len))\n",
        "# adding just one layer of lstm to test the approach\n",
        "model.add(LSTM(256, dropout = 0.1))\n",
        "#Add Output Layer\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag0U3DvZBT-z",
        "colab_type": "text"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt-kXnFrxtbh",
        "colab_type": "code",
        "outputId": "490ac541-4ced-4ca4-d45a-ac028d08aa52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "filepath=\"/content/drive/My Drive/EIP2/Best models/textGeneration-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# fit the model\n",
        "model.fit(predictors, label, epochs=100, batch_size=256, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "23904/23904 [==============================] - 5s 197us/step - loss: 6.4370\n",
            "\n",
            "Epoch 00001: loss improved from inf to 6.43696, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-01-6.4370-bigger.hdf5\n",
            "Epoch 2/100\n",
            "23904/23904 [==============================] - 3s 123us/step - loss: 6.0470\n",
            "\n",
            "Epoch 00002: loss improved from 6.43696 to 6.04704, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-02-6.0470-bigger.hdf5\n",
            "Epoch 3/100\n",
            "23904/23904 [==============================] - 3s 119us/step - loss: 5.9630\n",
            "\n",
            "Epoch 00003: loss improved from 6.04704 to 5.96295, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-03-5.9630-bigger.hdf5\n",
            "Epoch 4/100\n",
            "23904/23904 [==============================] - 3s 119us/step - loss: 5.8882\n",
            "\n",
            "Epoch 00004: loss improved from 5.96295 to 5.88816, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-04-5.8882-bigger.hdf5\n",
            "Epoch 5/100\n",
            "23904/23904 [==============================] - 3s 121us/step - loss: 5.8123\n",
            "\n",
            "Epoch 00005: loss improved from 5.88816 to 5.81232, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-05-5.8123-bigger.hdf5\n",
            "Epoch 6/100\n",
            "23904/23904 [==============================] - 3s 121us/step - loss: 5.7295\n",
            "\n",
            "Epoch 00006: loss improved from 5.81232 to 5.72951, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-06-5.7295-bigger.hdf5\n",
            "Epoch 7/100\n",
            "23904/23904 [==============================] - 3s 120us/step - loss: 5.6592\n",
            "\n",
            "Epoch 00007: loss improved from 5.72951 to 5.65919, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-07-5.6592-bigger.hdf5\n",
            "Epoch 8/100\n",
            "23904/23904 [==============================] - 3s 124us/step - loss: 5.6045\n",
            "\n",
            "Epoch 00008: loss improved from 5.65919 to 5.60455, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-08-5.6045-bigger.hdf5\n",
            "Epoch 9/100\n",
            "23904/23904 [==============================] - 3s 122us/step - loss: 5.5549\n",
            "\n",
            "Epoch 00009: loss improved from 5.60455 to 5.55489, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-09-5.5549-bigger.hdf5\n",
            "Epoch 10/100\n",
            "23904/23904 [==============================] - 3s 122us/step - loss: 5.5088\n",
            "\n",
            "Epoch 00010: loss improved from 5.55489 to 5.50885, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-10-5.5088-bigger.hdf5\n",
            "Epoch 11/100\n",
            "23904/23904 [==============================] - 3s 123us/step - loss: 5.4656\n",
            "\n",
            "Epoch 00011: loss improved from 5.50885 to 5.46559, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-11-5.4656-bigger.hdf5\n",
            "Epoch 12/100\n",
            "23904/23904 [==============================] - 3s 120us/step - loss: 5.4222\n",
            "\n",
            "Epoch 00012: loss improved from 5.46559 to 5.42225, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-12-5.4222-bigger.hdf5\n",
            "Epoch 13/100\n",
            "23904/23904 [==============================] - 3s 126us/step - loss: 5.3784\n",
            "\n",
            "Epoch 00013: loss improved from 5.42225 to 5.37841, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-13-5.3784-bigger.hdf5\n",
            "Epoch 14/100\n",
            "23904/23904 [==============================] - 3s 120us/step - loss: 5.3370\n",
            "\n",
            "Epoch 00014: loss improved from 5.37841 to 5.33704, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-14-5.3370-bigger.hdf5\n",
            "Epoch 15/100\n",
            "23904/23904 [==============================] - 3s 120us/step - loss: 5.2884\n",
            "\n",
            "Epoch 00015: loss improved from 5.33704 to 5.28836, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-15-5.2884-bigger.hdf5\n",
            "Epoch 16/100\n",
            "23904/23904 [==============================] - 3s 125us/step - loss: 5.2414\n",
            "\n",
            "Epoch 00016: loss improved from 5.28836 to 5.24139, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-16-5.2414-bigger.hdf5\n",
            "Epoch 17/100\n",
            "23904/23904 [==============================] - 3s 121us/step - loss: 5.1848\n",
            "\n",
            "Epoch 00017: loss improved from 5.24139 to 5.18480, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-17-5.1848-bigger.hdf5\n",
            "Epoch 18/100\n",
            "23904/23904 [==============================] - 3s 125us/step - loss: 5.1262\n",
            "\n",
            "Epoch 00018: loss improved from 5.18480 to 5.12617, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-18-5.1262-bigger.hdf5\n",
            "Epoch 19/100\n",
            "23904/23904 [==============================] - 3s 125us/step - loss: 5.0655\n",
            "\n",
            "Epoch 00019: loss improved from 5.12617 to 5.06554, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-19-5.0655-bigger.hdf5\n",
            "Epoch 20/100\n",
            "23904/23904 [==============================] - 3s 126us/step - loss: 5.0075\n",
            "\n",
            "Epoch 00020: loss improved from 5.06554 to 5.00747, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-20-5.0075-bigger.hdf5\n",
            "Epoch 21/100\n",
            "23904/23904 [==============================] - 3s 130us/step - loss: 4.9453\n",
            "\n",
            "Epoch 00021: loss improved from 5.00747 to 4.94534, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-21-4.9453-bigger.hdf5\n",
            "Epoch 22/100\n",
            "23904/23904 [==============================] - 3s 119us/step - loss: 4.8825\n",
            "\n",
            "Epoch 00022: loss improved from 4.94534 to 4.88252, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-22-4.8825-bigger.hdf5\n",
            "Epoch 23/100\n",
            "23904/23904 [==============================] - 3s 121us/step - loss: 4.8227\n",
            "\n",
            "Epoch 00023: loss improved from 4.88252 to 4.82271, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-23-4.8227-bigger.hdf5\n",
            "Epoch 24/100\n",
            "23904/23904 [==============================] - 3s 123us/step - loss: 4.7608\n",
            "\n",
            "Epoch 00024: loss improved from 4.82271 to 4.76079, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-24-4.7608-bigger.hdf5\n",
            "Epoch 25/100\n",
            "23904/23904 [==============================] - 3s 120us/step - loss: 4.6981\n",
            "\n",
            "Epoch 00025: loss improved from 4.76079 to 4.69809, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-25-4.6981-bigger.hdf5\n",
            "Epoch 26/100\n",
            "23904/23904 [==============================] - 3s 121us/step - loss: 4.6395\n",
            "\n",
            "Epoch 00026: loss improved from 4.69809 to 4.63955, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-26-4.6395-bigger.hdf5\n",
            "Epoch 27/100\n",
            "23904/23904 [==============================] - 3s 121us/step - loss: 4.5803\n",
            "\n",
            "Epoch 00027: loss improved from 4.63955 to 4.58034, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-27-4.5803-bigger.hdf5\n",
            "Epoch 28/100\n",
            "23904/23904 [==============================] - 3s 122us/step - loss: 4.5208\n",
            "\n",
            "Epoch 00028: loss improved from 4.58034 to 4.52084, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-28-4.5208-bigger.hdf5\n",
            "Epoch 29/100\n",
            "23904/23904 [==============================] - 3s 123us/step - loss: 4.4635\n",
            "\n",
            "Epoch 00029: loss improved from 4.52084 to 4.46353, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-29-4.4635-bigger.hdf5\n",
            "Epoch 30/100\n",
            "23904/23904 [==============================] - 3s 122us/step - loss: 4.4079\n",
            "\n",
            "Epoch 00030: loss improved from 4.46353 to 4.40789, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-30-4.4079-bigger.hdf5\n",
            "Epoch 31/100\n",
            "23904/23904 [==============================] - 3s 123us/step - loss: 4.3513\n",
            "\n",
            "Epoch 00031: loss improved from 4.40789 to 4.35128, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-31-4.3513-bigger.hdf5\n",
            "Epoch 32/100\n",
            "23904/23904 [==============================] - 3s 119us/step - loss: 4.2966\n",
            "\n",
            "Epoch 00032: loss improved from 4.35128 to 4.29662, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-32-4.2966-bigger.hdf5\n",
            "Epoch 33/100\n",
            "23904/23904 [==============================] - 3s 123us/step - loss: 4.2450\n",
            "\n",
            "Epoch 00033: loss improved from 4.29662 to 4.24503, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-33-4.2450-bigger.hdf5\n",
            "Epoch 34/100\n",
            "23904/23904 [==============================] - 3s 122us/step - loss: 4.1928\n",
            "\n",
            "Epoch 00034: loss improved from 4.24503 to 4.19280, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-34-4.1928-bigger.hdf5\n",
            "Epoch 35/100\n",
            "23904/23904 [==============================] - 3s 124us/step - loss: 4.1406\n",
            "\n",
            "Epoch 00035: loss improved from 4.19280 to 4.14057, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-35-4.1406-bigger.hdf5\n",
            "Epoch 36/100\n",
            "23904/23904 [==============================] - 3s 123us/step - loss: 4.0909\n",
            "\n",
            "Epoch 00036: loss improved from 4.14057 to 4.09090, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-36-4.0909-bigger.hdf5\n",
            "Epoch 37/100\n",
            "23904/23904 [==============================] - 3s 124us/step - loss: 4.0409\n",
            "\n",
            "Epoch 00037: loss improved from 4.09090 to 4.04093, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-37-4.0409-bigger.hdf5\n",
            "Epoch 38/100\n",
            "23904/23904 [==============================] - 3s 128us/step - loss: 3.9918\n",
            "\n",
            "Epoch 00038: loss improved from 4.04093 to 3.99176, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-38-3.9918-bigger.hdf5\n",
            "Epoch 39/100\n",
            "23904/23904 [==============================] - 3s 124us/step - loss: 3.9432\n",
            "\n",
            "Epoch 00039: loss improved from 3.99176 to 3.94318, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-39-3.9432-bigger.hdf5\n",
            "Epoch 40/100\n",
            "23904/23904 [==============================] - 3s 125us/step - loss: 3.8970\n",
            "\n",
            "Epoch 00040: loss improved from 3.94318 to 3.89703, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-40-3.8970-bigger.hdf5\n",
            "Epoch 41/100\n",
            "23904/23904 [==============================] - 3s 125us/step - loss: 3.8506\n",
            "\n",
            "Epoch 00041: loss improved from 3.89703 to 3.85065, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-41-3.8506-bigger.hdf5\n",
            "Epoch 42/100\n",
            "23904/23904 [==============================] - 3s 119us/step - loss: 3.8067\n",
            "\n",
            "Epoch 00042: loss improved from 3.85065 to 3.80672, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-42-3.8067-bigger.hdf5\n",
            "Epoch 43/100\n",
            "23904/23904 [==============================] - 3s 122us/step - loss: 3.7634\n",
            "\n",
            "Epoch 00043: loss improved from 3.80672 to 3.76344, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-43-3.7634-bigger.hdf5\n",
            "Epoch 44/100\n",
            "23904/23904 [==============================] - 3s 120us/step - loss: 3.7195\n",
            "\n",
            "Epoch 00044: loss improved from 3.76344 to 3.71953, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-44-3.7195-bigger.hdf5\n",
            "Epoch 45/100\n",
            "23904/23904 [==============================] - 3s 126us/step - loss: 3.6766\n",
            "\n",
            "Epoch 00045: loss improved from 3.71953 to 3.67664, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-45-3.6766-bigger.hdf5\n",
            "Epoch 46/100\n",
            "23904/23904 [==============================] - 3s 120us/step - loss: 3.6331\n",
            "\n",
            "Epoch 00046: loss improved from 3.67664 to 3.63309, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-46-3.6331-bigger.hdf5\n",
            "Epoch 47/100\n",
            "23904/23904 [==============================] - 3s 119us/step - loss: 3.5921\n",
            "\n",
            "Epoch 00047: loss improved from 3.63309 to 3.59211, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-47-3.5921-bigger.hdf5\n",
            "Epoch 48/100\n",
            "23904/23904 [==============================] - 3s 123us/step - loss: 3.5521\n",
            "\n",
            "Epoch 00048: loss improved from 3.59211 to 3.55211, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-48-3.5521-bigger.hdf5\n",
            "Epoch 49/100\n",
            "23904/23904 [==============================] - 3s 128us/step - loss: 3.5113\n",
            "\n",
            "Epoch 00049: loss improved from 3.55211 to 3.51126, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-49-3.5113-bigger.hdf5\n",
            "Epoch 50/100\n",
            "23904/23904 [==============================] - 3s 131us/step - loss: 3.4734\n",
            "\n",
            "Epoch 00050: loss improved from 3.51126 to 3.47339, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-50-3.4734-bigger.hdf5\n",
            "Epoch 51/100\n",
            "23904/23904 [==============================] - 3s 130us/step - loss: 3.4359\n",
            "\n",
            "Epoch 00051: loss improved from 3.47339 to 3.43594, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-51-3.4359-bigger.hdf5\n",
            "Epoch 52/100\n",
            "23904/23904 [==============================] - 3s 122us/step - loss: 3.3999\n",
            "\n",
            "Epoch 00052: loss improved from 3.43594 to 3.39993, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-52-3.3999-bigger.hdf5\n",
            "Epoch 53/100\n",
            "23904/23904 [==============================] - 3s 126us/step - loss: 3.3621\n",
            "\n",
            "Epoch 00053: loss improved from 3.39993 to 3.36213, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-53-3.3621-bigger.hdf5\n",
            "Epoch 54/100\n",
            "23904/23904 [==============================] - 3s 121us/step - loss: 3.3246\n",
            "\n",
            "Epoch 00054: loss improved from 3.36213 to 3.32456, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-54-3.3246-bigger.hdf5\n",
            "Epoch 55/100\n",
            "23904/23904 [==============================] - 3s 121us/step - loss: 3.2910\n",
            "\n",
            "Epoch 00055: loss improved from 3.32456 to 3.29105, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-55-3.2910-bigger.hdf5\n",
            "Epoch 56/100\n",
            "23904/23904 [==============================] - 3s 121us/step - loss: 3.2557\n",
            "\n",
            "Epoch 00056: loss improved from 3.29105 to 3.25567, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-56-3.2557-bigger.hdf5\n",
            "Epoch 57/100\n",
            "23904/23904 [==============================] - 3s 121us/step - loss: 3.2201\n",
            "\n",
            "Epoch 00057: loss improved from 3.25567 to 3.22012, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-57-3.2201-bigger.hdf5\n",
            "Epoch 58/100\n",
            "23904/23904 [==============================] - 3s 128us/step - loss: 3.1868\n",
            "\n",
            "Epoch 00058: loss improved from 3.22012 to 3.18682, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-58-3.1868-bigger.hdf5\n",
            "Epoch 59/100\n",
            "23904/23904 [==============================] - 3s 127us/step - loss: 3.1562\n",
            "\n",
            "Epoch 00059: loss improved from 3.18682 to 3.15624, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-59-3.1562-bigger.hdf5\n",
            "Epoch 60/100\n",
            "23904/23904 [==============================] - 3s 130us/step - loss: 3.1228\n",
            "\n",
            "Epoch 00060: loss improved from 3.15624 to 3.12284, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-60-3.1228-bigger.hdf5\n",
            "Epoch 61/100\n",
            "23904/23904 [==============================] - 3s 123us/step - loss: 3.0925\n",
            "\n",
            "Epoch 00061: loss improved from 3.12284 to 3.09250, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-61-3.0925-bigger.hdf5\n",
            "Epoch 62/100\n",
            "23904/23904 [==============================] - 3s 122us/step - loss: 3.0599\n",
            "\n",
            "Epoch 00062: loss improved from 3.09250 to 3.05995, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-62-3.0599-bigger.hdf5\n",
            "Epoch 63/100\n",
            "23904/23904 [==============================] - 3s 123us/step - loss: 3.0302\n",
            "\n",
            "Epoch 00063: loss improved from 3.05995 to 3.03020, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-63-3.0302-bigger.hdf5\n",
            "Epoch 64/100\n",
            "23904/23904 [==============================] - 3s 128us/step - loss: 2.9989\n",
            "\n",
            "Epoch 00064: loss improved from 3.03020 to 2.99887, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-64-2.9989-bigger.hdf5\n",
            "Epoch 65/100\n",
            "23904/23904 [==============================] - 3s 122us/step - loss: 2.9685\n",
            "\n",
            "Epoch 00065: loss improved from 2.99887 to 2.96848, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-65-2.9685-bigger.hdf5\n",
            "Epoch 66/100\n",
            "23904/23904 [==============================] - 3s 125us/step - loss: 2.9421\n",
            "\n",
            "Epoch 00066: loss improved from 2.96848 to 2.94214, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-66-2.9421-bigger.hdf5\n",
            "Epoch 67/100\n",
            "23904/23904 [==============================] - 3s 130us/step - loss: 2.9116\n",
            "\n",
            "Epoch 00067: loss improved from 2.94214 to 2.91160, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-67-2.9116-bigger.hdf5\n",
            "Epoch 68/100\n",
            "23904/23904 [==============================] - 3s 130us/step - loss: 2.8824\n",
            "\n",
            "Epoch 00068: loss improved from 2.91160 to 2.88241, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-68-2.8824-bigger.hdf5\n",
            "Epoch 69/100\n",
            "23904/23904 [==============================] - 3s 127us/step - loss: 2.8547\n",
            "\n",
            "Epoch 00069: loss improved from 2.88241 to 2.85470, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-69-2.8547-bigger.hdf5\n",
            "Epoch 70/100\n",
            "23904/23904 [==============================] - 3s 122us/step - loss: 2.8296\n",
            "\n",
            "Epoch 00070: loss improved from 2.85470 to 2.82959, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-70-2.8296-bigger.hdf5\n",
            "Epoch 71/100\n",
            "23904/23904 [==============================] - 3s 126us/step - loss: 2.8014\n",
            "\n",
            "Epoch 00071: loss improved from 2.82959 to 2.80144, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-71-2.8014-bigger.hdf5\n",
            "Epoch 72/100\n",
            "23904/23904 [==============================] - 3s 124us/step - loss: 2.7746\n",
            "\n",
            "Epoch 00072: loss improved from 2.80144 to 2.77456, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-72-2.7746-bigger.hdf5\n",
            "Epoch 73/100\n",
            "23904/23904 [==============================] - 3s 126us/step - loss: 2.7503\n",
            "\n",
            "Epoch 00073: loss improved from 2.77456 to 2.75027, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-73-2.7503-bigger.hdf5\n",
            "Epoch 74/100\n",
            "23904/23904 [==============================] - 3s 127us/step - loss: 2.7234\n",
            "\n",
            "Epoch 00074: loss improved from 2.75027 to 2.72343, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-74-2.7234-bigger.hdf5\n",
            "Epoch 75/100\n",
            "23904/23904 [==============================] - 3s 127us/step - loss: 2.6981\n",
            "\n",
            "Epoch 00075: loss improved from 2.72343 to 2.69810, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-75-2.6981-bigger.hdf5\n",
            "Epoch 76/100\n",
            "23904/23904 [==============================] - 3s 134us/step - loss: 2.6737\n",
            "\n",
            "Epoch 00076: loss improved from 2.69810 to 2.67372, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-76-2.6737-bigger.hdf5\n",
            "Epoch 77/100\n",
            "23904/23904 [==============================] - 3s 129us/step - loss: 2.6469\n",
            "\n",
            "Epoch 00077: loss improved from 2.67372 to 2.64688, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-77-2.6469-bigger.hdf5\n",
            "Epoch 78/100\n",
            "23904/23904 [==============================] - 3s 123us/step - loss: 2.6220\n",
            "\n",
            "Epoch 00078: loss improved from 2.64688 to 2.62196, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-78-2.6220-bigger.hdf5\n",
            "Epoch 79/100\n",
            "23904/23904 [==============================] - 3s 125us/step - loss: 2.5961\n",
            "\n",
            "Epoch 00079: loss improved from 2.62196 to 2.59614, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-79-2.5961-bigger.hdf5\n",
            "Epoch 80/100\n",
            "23904/23904 [==============================] - 3s 129us/step - loss: 2.5758\n",
            "\n",
            "Epoch 00080: loss improved from 2.59614 to 2.57576, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-80-2.5758-bigger.hdf5\n",
            "Epoch 81/100\n",
            "23904/23904 [==============================] - 3s 127us/step - loss: 2.5509\n",
            "\n",
            "Epoch 00081: loss improved from 2.57576 to 2.55092, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-81-2.5509-bigger.hdf5\n",
            "Epoch 82/100\n",
            "23904/23904 [==============================] - 3s 124us/step - loss: 2.5274\n",
            "\n",
            "Epoch 00082: loss improved from 2.55092 to 2.52740, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-82-2.5274-bigger.hdf5\n",
            "Epoch 83/100\n",
            "23904/23904 [==============================] - 3s 122us/step - loss: 2.5049\n",
            "\n",
            "Epoch 00083: loss improved from 2.52740 to 2.50485, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-83-2.5049-bigger.hdf5\n",
            "Epoch 84/100\n",
            "23904/23904 [==============================] - 3s 123us/step - loss: 2.4799\n",
            "\n",
            "Epoch 00084: loss improved from 2.50485 to 2.47985, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-84-2.4799-bigger.hdf5\n",
            "Epoch 85/100\n",
            "23904/23904 [==============================] - 3s 123us/step - loss: 2.4592\n",
            "\n",
            "Epoch 00085: loss improved from 2.47985 to 2.45916, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-85-2.4592-bigger.hdf5\n",
            "Epoch 86/100\n",
            "23904/23904 [==============================] - 3s 123us/step - loss: 2.4377\n",
            "\n",
            "Epoch 00086: loss improved from 2.45916 to 2.43772, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-86-2.4377-bigger.hdf5\n",
            "Epoch 87/100\n",
            "23904/23904 [==============================] - 3s 130us/step - loss: 2.4136\n",
            "\n",
            "Epoch 00087: loss improved from 2.43772 to 2.41356, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-87-2.4136-bigger.hdf5\n",
            "Epoch 88/100\n",
            "23904/23904 [==============================] - 3s 127us/step - loss: 2.3902\n",
            "\n",
            "Epoch 00088: loss improved from 2.41356 to 2.39017, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-88-2.3902-bigger.hdf5\n",
            "Epoch 89/100\n",
            "23904/23904 [==============================] - 3s 129us/step - loss: 2.3722\n",
            "\n",
            "Epoch 00089: loss improved from 2.39017 to 2.37225, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-89-2.3722-bigger.hdf5\n",
            "Epoch 90/100\n",
            "23904/23904 [==============================] - 3s 128us/step - loss: 2.3488\n",
            "\n",
            "Epoch 00090: loss improved from 2.37225 to 2.34883, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-90-2.3488-bigger.hdf5\n",
            "Epoch 91/100\n",
            "23904/23904 [==============================] - 3s 128us/step - loss: 2.3295\n",
            "\n",
            "Epoch 00091: loss improved from 2.34883 to 2.32954, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-91-2.3295-bigger.hdf5\n",
            "Epoch 92/100\n",
            "23904/23904 [==============================] - 3s 131us/step - loss: 2.3066\n",
            "\n",
            "Epoch 00092: loss improved from 2.32954 to 2.30658, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-92-2.3066-bigger.hdf5\n",
            "Epoch 93/100\n",
            "23904/23904 [==============================] - 3s 129us/step - loss: 2.2859\n",
            "\n",
            "Epoch 00093: loss improved from 2.30658 to 2.28590, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-93-2.2859-bigger.hdf5\n",
            "Epoch 94/100\n",
            "23904/23904 [==============================] - 3s 129us/step - loss: 2.2669\n",
            "\n",
            "Epoch 00094: loss improved from 2.28590 to 2.26691, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-94-2.2669-bigger.hdf5\n",
            "Epoch 95/100\n",
            "23904/23904 [==============================] - 3s 123us/step - loss: 2.2482\n",
            "\n",
            "Epoch 00095: loss improved from 2.26691 to 2.24824, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-95-2.2482-bigger.hdf5\n",
            "Epoch 96/100\n",
            "23904/23904 [==============================] - 3s 127us/step - loss: 2.2271\n",
            "\n",
            "Epoch 00096: loss improved from 2.24824 to 2.22710, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-96-2.2271-bigger.hdf5\n",
            "Epoch 97/100\n",
            "23904/23904 [==============================] - 3s 129us/step - loss: 2.2056\n",
            "\n",
            "Epoch 00097: loss improved from 2.22710 to 2.20556, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-97-2.2056-bigger.hdf5\n",
            "Epoch 98/100\n",
            "23904/23904 [==============================] - 3s 131us/step - loss: 2.1861\n",
            "\n",
            "Epoch 00098: loss improved from 2.20556 to 2.18612, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-98-2.1861-bigger.hdf5\n",
            "Epoch 99/100\n",
            "23904/23904 [==============================] - 3s 120us/step - loss: 2.1662\n",
            "\n",
            "Epoch 00099: loss improved from 2.18612 to 2.16623, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-99-2.1662-bigger.hdf5\n",
            "Epoch 100/100\n",
            "23904/23904 [==============================] - 3s 124us/step - loss: 2.1486\n",
            "\n",
            "Epoch 00100: loss improved from 2.16623 to 2.14859, saving model to /content/drive/My Drive/EIP2/Best models/textGeneration-100-2.1486-bigger.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0f8d4e9e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujwXMLAEBY0b",
        "colab_type": "text"
      },
      "source": [
        "## Generating the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Afsf5gTDBsYa",
        "colab_type": "text"
      },
      "source": [
        "Generating 500 words from any seed chosen at random from the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0b6yHR11Xkj",
        "colab_type": "code",
        "outputId": "8244bab2-c9a4-48bb-8796-2f697724687a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "seed_text = \"making faces at him as he spoke\"\n",
        "for _ in range(500):\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "  predicted = model.predict_classes(token_list, verbose=0)\n",
        "\n",
        "  output_word = \"\"\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "      if index == predicted:\n",
        "          output_word = word\n",
        "          break\n",
        "  seed_text += \" \"+output_word\n",
        "print(seed_text.title().lower())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "making faces at him as he spoke to get in the clock in a well air it was talking the list of the deepest as the rest of the table as it was his feet high he felt all the list of expecting to begin with her to settle as she spoke and the moral of the song the king was leaning with the wood and the white rabbit blew three blasts on the trumpet and then he would be the right thing said the mock turtle washingextra disagree with a great hurry and the fan and was suppressed under the little drawlingthe key and unlocking the door and the pair of the gloves the king went on the gloves and the gloves he was speaking to rightly as she was beginning to rightly sure i am i think to be sure i am to be otherwise messages wondering but the officer could get in a court sometimes choked with the pope civil in a minute or kettle was all the list of the deepest as the moment the king added swim the gloves she had caught the hatter and the hatter hurriedly left in the sun and had just in the same head as he fumbled over the list of the creature and the queen was busily up and the moment he was gone and it was over to get in a melancholy livery had just just just at the hatter i mean what a nice little thing howled along the gryphon and the hatter was over with a great hurry in the wood and the pair of the suppressed guineapigs instantly and all the queen was in the direction the king added swim in a candle she had caught the flamingo and he poured a little wider about a trumpet and then she had not to talk in any rate she had never to sing you join the dance and in a minute or two she walked up and the dormouse was over with a great hurry in sneezes energetic it was beginning won and she tried to curtsey in a few minutes which seemed to be full of soup and dry leaves to be sure i am i think to be otherwise among the glass table and i have what to lie down on it and she had not to me so i cant get in a natural livery came trotting along the queen just in a pie was a real tale and the hatter was over with a great hurry in the wood and the pair of the suppressed guineapigs instantly and all the queen was in the direction the king added swim in a candle she had caught the flamingo and he poured a little wider about a trumpet and then she had not to talk in any rate she had never to sing you join the dance and in a minute or two she walked up and the dormouse was over with a great hurry in sneezes energetic it was beginning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3W8j0Kf8eA0",
        "colab_type": "text"
      },
      "source": [
        "<b>seed</b>:making faces at him as he spoke\n",
        "\n",
        "\n",
        "<b>Generated text</b>: to get in the clock in a well air it was talking the list of the deepest as the rest of the table as it was his feet high he felt all the list of expecting to begin with her to settle as she spoke and the moral of the song the king was leaning with the wood and the white rabbit blew three blasts on the trumpet and then he would be the right thing said the mock turtle washingextra disagree with a great hurry and the fan and was suppressed under the little drawlingthe key and unlocking the door and the pair of the gloves the king went on the gloves and the gloves he was speaking to rightly as she was beginning to rightly sure i am i think to be sure i am to be otherwise messages wondering but the officer could get in a court sometimes choked with the pope civil in a minute or kettle was all the list of the deepest as the moment the king added swim the gloves she had caught the hatter and the hatter hurriedly left in the sun and had just in the same head as he fumbled over the list of the creature and the queen was busily up and the moment he was gone and it was over to get in a melancholy livery had just just just at the hatter i mean what a nice little thing howled along the gryphon and the hatter was over with a great hurry in the wood and the pair of the suppressed guineapigs instantly and all the queen was in the direction the king added swim in a candle she had caught the flamingo and he poured a little wider about a trumpet and then she had not to talk in any rate she had never to sing you join the dance and in a minute or two she walked up and the dormouse was over with a great hurry in sneezes energetic it was beginning won and she tried to curtsey in a few minutes which seemed to be full of soup and dry leaves to be sure i am i think to be otherwise among the glass table and i have what to lie down on it and she had not to me so i cant get in a natural livery came trotting along the queen just in a pie was a real tale and the hatter was over with a great hurry in the wood and the pair of the suppressed guineapigs instantly and all the queen was in the direction the king added swim in a candle she had caught the flamingo and he poured a little wider about a trumpet and then she had not to talk in any rate she had never to sing you join the dance and in a minute or two she walked up and the dormouse was over with a great hurry in sneezes energetic it was beginning"
      ]
    }
  ]
}